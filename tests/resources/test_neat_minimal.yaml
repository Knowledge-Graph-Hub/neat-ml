---
output_directory: tests/resources/test_output_data_dir/

graph_data:
  graph:
    directed: False
    node_path: tests/resources/test_graphs/pos_train_nodes.tsv
    edge_path: tests/resources/test_graphs/pos_train_edges.tsv
    verbose: True
    nodes_column: 'id'
    node_list_node_types_column: 'category'
    default_node_type: 'biolink:NamedThing'
    sources_column: 'subject'
    destinations_column: 'object'
    default_edge_type: 'biolink:related_to'

  #
  # classifier-specific graphs:
  #
  # (when making a classifier, positive training graph is assumed to be 'graph' above)
  # all params below override those in 'graph' above
  pos_validation:
    edge_path: tests/resources/test_graphs/pos_valid_edges.tsv
  neg_training:
    edge_path: tests/resources/test_graphs/neg_train_edges.tsv
  neg_validation:
    edge_path: tests/resources/test_graphs/neg_valid_edges.tsv

embeddings:
  embedding_file_name: test_embeddings_test_yaml.csv
  embedding_history_file_name: embedding_history.json
  node_embedding_params:
      node_embedding_method_name: SkipGram # one of 'CBOW', 'GloVe', 'SkipGram', 'Siamese', 'TransE', 'SimplE', 'TransH', 'TransR'
      # graph: Graph,
      # node_embedding_method_name: str,
      # use_mirrored_strategy: bool = True,
      # devices: Union[List[str], str] = None,
      # fit_kwargs: Dict = None,
      # verbose: Union[bool, int] = True,
      # automatically_drop_unsupported_parameters: bool = False,
      # automatically_enable_time_memory_tradeoffs: bool = True,
      # automatically_sort_by_decreasing_outbound_node_degree: bool = True,
      # ** kwargs: Dict
      walk_length: 10 # typically 100 or so
      batch_size: 128 # typically 512? or more
      window_size: 4
      return_weight: 1.0  # 1/p
      explore_weight: 1.0  # 1/q
      iterations: 5 # typically 20

  tsne:
    tsne_file_name: tsne.png

classifiers:
  -
    classifier_id: lr_0
    type: Logistic Regression
    edge_method: Average # one of EdgeTransformer.methods: Hadamard, Sum, Average, L1, AbsoluteL1, L2, or alternatively a lambda
    model:
      outfile: "model_lr_test_yaml.h5"
      type: sklearn.linear_model.LogisticRegression
      parameters:
        random_state: 42
        max_iter: 100

  -
    classifier_id: mlp_0
    type: neural network
    edge_method: Average # one of EdgeTransformer.methods: Hadamard, Sum, Average, L1, AbsoluteL1, L2, or alternatively a lambda
    model:
      outfile: "model_mlp_test_yaml.h5"
      classifier_history_file_name: "mlp_classifier_history.json"
      type: tensorflow.keras.models.Sequential
      layers:
        - type: tensorflow.keras.layers.Input
          parameters:
            shape: 100   # must match embedding_size up above
        - type: tensorflow.keras.layers.Dense
          parameters:
            units: 128
            activation: relu
        - type: tensorflow.keras.layers.Dense
          parameters:
            units: 32
            activation: relu
            # TODO: fix this:
            # activity_regularizer: tensorflow.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)
        - type: tensorflow.keras.layers.Dropout
          parameters:
            rate: 0.5
        - type: tensorflow.keras.layers.Dense
          parameters:
            units: 16
            activation: relu
        - type: tensorflow.keras.layers.Dense
          parameters:
            units: 1
            activation: sigmoid
    model_compile:
      loss: binary_crossentropy
      optimizer: nadam
      metrics:
        # these can be tensorflow objects
        # or a string that tensorflow understands,
        # e.g. 'accuracy'
        - type: tensorflow.keras.metrics.AUC
          parameters:
            curve: PR
            name: auprc
        - type: tensorflow.keras.metrics.AUC
          parameters:
            curve: ROC
            name: auroc
        - type: tensorflow.keras.metrics.Recall
          parameters:
            name: Recall
        - type: tensorflow.keras.metrics.Precision
          parameters:
            name: Precision
        - type: accuracy
    model_fit:
      parameters:
        batch_size: 4096
        epochs: 5  # typically much higher
        callbacks:
          - type: tensorflow.keras.callbacks.EarlyStopping
            parameters:
              monitor: val_loss
              patience: 5
              min_delta: 0.001  # min improvement to be considered progress
          - type: tensorflow.keras.callbacks.ReduceLROnPlateau

apply_trained_classifier:
  -
    classifier_model_id: lr_0
    link_node_types: #Note that these source/destination types are not in the input graph!
      source:
          - 'biolink:Drug'
          - 'biolink:ChemicalSubstance'
      destination:
          - 'biolink:Protein'
    cutoff: 0.9
    outfile: lr_classifier_predictions_kgx.tsv
  -
    classifier_model_id: mlp_0
    link_node_types: #Note that these source/destination types are not in the input graph!
        source:
          - 'biolink:Drug'
          - 'biolink:ChemicalSubstance'
        destination:
          - 'biolink:Protein'
    cutoff: 0.9
    outfile: mlp_classifier_predictions_kgx.tsv
