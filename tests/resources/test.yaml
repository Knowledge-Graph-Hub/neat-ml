---
name: ""
description: ""
output_directory: output_data

graph_data:
  graph:
    directed: False
    verbose: True
    nodes_column: 'id'
    node_types_column: 'category'
    default_node_type: 'biolink:NamedThing'
    # other parameters:
    # nodes_column_number: int = None
    # node_types_column_number: int = None,
    # node_header: optional str
    # node_rows_to_skip: int = 0
    # node_separator: str = "\t"
    sources_column: 'subject'
    destinations_column: 'object'
    # edge_types_column: 'weight'  # fix this
    default_edge_type: 'biolink:related_to'
    node_path: tests/resources/test_graphs/pos_train_nodes.tsv
    edge_path: tests/resources/test_graphs/pos_train_edges.tsv
    # edge_header: str
    # edge_rows_to_skip: int = 0,
    # sources_column_number: int = 0
    # destinations_column_number: int = 1
    # edge_types_column_number: int = None
    # edge_separator: str = "\t"
    ### weight stuff
    # weights_column: 'weight'
    # default_weight: 1
    # weights_column_number: int = None

  #
  # classifier-specific graphs:
  #
  # (when making a classifier, positive training graph is assumed to be 'graph' above)
  # all params below override those in 'graph' above
  predefined_holdouts:
    pos_validation:
      edge_path: tests/resources/test_graphs/pos_valid_edges.tsv
    neg_training:
      edge_path: tests/resources/test_graphs/neg_train_edges.tsv
    neg_validation:
      edge_path: tests/resources/test_graphs/neg_valid_edges.tsv
  # OR
  # ensmallen_holdouts:
    # params for holdouts, passed to Ensmallen
    # need to save holdouts as artefacts probably

embeddings:
  resume: True
  embedding_file_name: test_embeddings_test_yaml.tsv
  embedding_history_file_name: embedding_history.json

  use_pos_valid_for_early_stopping: True # if True, must specify pos_validation_graph above
  embiggen_params:
    epochs: 1 # typically more than this
    seq_params:
      node_embedding_method_name: SkipGram # one of 'CBOW', 'GloVe', 'SkipGram', 'Siamese', 'TransE', 'SimplE', 'TransH', 'TransR'
      walk_length: 10 # typically 100 or so
      batch_size: 128 # typically 512? or more
      window_size: 4
      return_weight: 1.0  # 1/p
      explore_weight: 1.0  # 1/q
      iterations: 5 # typically 20
    node2vec_params:  # these params are passed to SkipGram() or CBOW()
      embedding_size: 100
      negative_samples: 30
    early_stopping:  # optional
      patience: 5
      min_delta: 0.0001
      restore_best_weights: True
      monitor: loss
    optimizer:  # hard-coded to Nadam for now
      learning_rate: 0.1
  bert_params:
    node_columns:  # ideally these would have more text than category or ID (description)
      - category
      - id
  metrics:  # these can be tensorflow.keras.metrics
    - type: tensorflow.keras.metrics.AUC
      parameters:
        name: AUC
    - type: tensorflow.keras.metrics.Recall
      parameters:
        name: Recall
    - type: tensorflow.keras.metrics.Precision
      parameters:
        name: Precision

classifier:
  edge_method: Average # one of EdgeTransformer.methods: Hadamard, Sum, Average, L1, AbsoluteL1, L2, or alternatively a lambda
  classifiers:  # a list of classifiers to be trained
    - type: neural network
      model:
        outfile: "model_mlp_test_yaml.h5"
        classifier_history_file_name: "mlp_classifier_history.json"
        type: tensorflow.keras.models.Sequential
        layers:
          - type: tensorflow.keras.layers.Input
            parameters:
              shape: 100   # must match embedding_size up above
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 128
              activation: relu
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 32
              activation: relu
              # TODO: fix this:
              # activity_regularizer: tensorflow.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)
          - type: tensorflow.keras.layers.Dropout
            parameters:
              rate: 0.5
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 16
              activation: relu
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 1
              activation: sigmoid
      model_compile:
        loss: binary_crossentropy
        optimizer: nadam
        metrics:  # these can be tensorflow objects or a string that tensorflow understands, e.g. 'accuracy'
          - type: tensorflow.keras.metrics.AUC
            parameters:
              curve: PR
              name: auprc
          - type: tensorflow.keras.metrics.AUC
            parameters:
              curve: ROC
              name: auroc
          - type: tensorflow.keras.metrics.Recall
            parameters:
              name: Recall
          - type: tensorflow.keras.metrics.Precision
            parameters:
              name: Precision
          - type: accuracy
      model_fit:
        parameters:
          batch_size: 4096
          epochs: 5  # typically much higher
          callbacks:
            - type: tensorflow.keras.callbacks.EarlyStopping
              parameters:
                monitor: val_loss
                patience: 5
                min_delta: 0.001  # min improvement to be considered progress
            - type: tensorflow.keras.callbacks.ReduceLROnPlateau
    - type: Decision Tree
      model:
        outfile: "model_decision_tree_test_yaml.h5"
        type: sklearn.tree.DecisionTreeClassifier
        parameters:
          max_depth: 30
          random_state: 42
    - type: Random Forest
      model:
        outfile: "model_random_forest_test_yaml.h5"
        type: sklearn.ensemble.RandomForestClassifier
        parameters:
          n_estimators: 500
          max_depth: 30
          n_jobs: 8  # cpu count
          random_state: 42
    - type: Logistic Regression
      model:
        outfile: "model_lr_test_yaml.h5"
        type: sklearn.linear_model.LogisticRegression
        parameters:
          random_state: 42
          max_iter: 1000

