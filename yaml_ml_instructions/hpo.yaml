---
name: ""
description: ""
output_directory: output_data

graph_data:
  graph:
    directed: False
    verbose: True
    skip_self_loops: False
    nodes_column: 'id'
    node_list_node_types_column: 'category'
    default_node_type: 'biolink:NamedThing'
    ignore_duplicated_nodes: True
    sources_column: 'subject'
    destinations_column: 'object'
    edge_types_column: 'label'
    default_edge_type: 'biolink:related_to'
    ignore_duplicated_edges: True
    node_path: input_data/biolink:subclass_of/hpo_nodes_training.tsv
    edge_path: input_data/biolink:subclass_of/hpo_edges_training.tsv

  pos_validation:
    edge_path: input_data/biolink:subclass_of/hpo_edges_validation.tsv
  neg_training:
    edge_path: input_data/biolink:subclass_of/hpo_edges_neg_training.tsv
    default_edge_type: 'negative_edge'
  neg_validation:
    edge_path: input_data/biolink:subclass_of/hpo_edges_neg_validation.tsv
    default_edge_type: 'negative_edge'

embeddings:
  resume: True
  embedding_file_name: SkipGram_training_graph_HPO.npy
  model_file_name: embedding_model_HPO.h5
  use_pos_valid_for_early_stopping: True # if True, must specify pos_validation_graph above
  tsne:  # comment this out if you don't want a TSNE plot
    n: 230 # how many processors to use
    tsne_file_name: hpo_tsne.png
    node_property_for_color: category
    scatter_params:
      marker: '.'
      s: 0.1
  embiggen_params:
    epochs: 100 # typically more than this
    seq_params:  # these params are passed to Node2VecSequence()
      walk_length: 100
      batch_size: 512
      window_size: 4
      return_weight: 1.0  # 1/p
      explore_weight: 1.0  # 1/q
      iterations: 20
    model: skipgram  # or CBOW
    node2vec_params:  # these params are passed to SkipGram() or CBOW()
      embedding_size: 100
      negative_samples: 30
    early_stopping:  # optional
      patience: 5
      min_delta: 0.0001
      restore_best_weights: True
      monitor: loss
    optimizer:  # hard-coded to Nadam for now
      learning_rate: 0.1


classifier:
  edge_method: average # one of EdgeTransformer.methods: hadamard, average, weightedL1, weightedL2
  classifiers:  # a list of classifiers to be trained
    - type: neural network
      model:
        outfile: "model_mlp_HPO.h5"
        type: tensorflow.keras.models.Sequential
        layers:
          - type: tensorflow.keras.layers.Input
            parameters:
              shape: 100   # must match embedding_size up above
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 128
              activation: relu
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 32
              activation: relu
              # TODO: fix this:
              # activity_regularizer: tensorflow.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)
          - type: tensorflow.keras.layers.Dropout
            parameters:
              rate: 0.5
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 16
              activation: relu
          - type: tensorflow.keras.layers.Dense
            parameters:
              units: 1
              activation: sigmoid
      model_compile:
        loss: binary_crossentropy
        optimizer: nadam
        metrics:  # these can be tensorflow objects or a string that tensorflow understands, e.g. 'accuracy'
          - type: tensorflow.keras.metrics.AUC
            parameters:
              curve: PR
              name: auprc
          - type: tensorflow.keras.metrics.AUC
            parameters:
              curve: ROC
              name: auroc
          - type: tensorflow.keras.metrics.Recall
            parameters:
              name: Recall
          - type: tensorflow.keras.metrics.Precision
            parameters:
              name: Precision
          - type: accuracy
      model_fit:
        parameters:
          batch_size: 4096  # don't use exponent notation (e.g. 2**12), it will be interpreted as a string
          epochs: 5  # typically much higher
          callbacks:
            - type: tensorflow.keras.callbacks.EarlyStopping
              parameters:
                monitor: val_loss
                patience: 5
                min_delta: 0.001  # min improvement to be considered progres
            - type: tensorflow.keras.callbacks.ReduceLROnPlateau
    - type: Decision Tree
      model:
        outfile: "model_decision_tree_HPO.h5"
        type: sklearn.tree.DecisionTreeClassifier
        parameters:
          max_depth: 30
          random_state: 42
    - type: Random Forest
      model:
        outfile: "model_random_forest_HPO.h5"
        type: sklearn.ensemble.RandomForestClassifier
        parameters:
          n_estimators: 500
          max_depth: 30
          n_jobs: 8  # cpu count
          random_state: 42
    - type: Logistic Regression
      model:
        outfile: "model_lr_HPO.h5"
        type: sklearn.linear_model.LogisticRegression
        parameters:
          random_state: 42
          max_iter: 1000
